{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CM50268 / CM30322\n",
    "\n",
    "## Coursework 1\n",
    "\n",
    "# Exploring Bayesian Linear Regression\n",
    "\n",
    "**Total Marks 30 (30% of overall unit grade).**\n",
    "\n",
    "*Submission deadline: 8pm, Friday 7th March. Please submit your completed notebook file in Moodle.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission & Marking\n",
    "\n",
    "For this coursework there are a number of places where you are expected to enter your own code. \n",
    "Every place you have to add code is  indicated by:\n",
    "\n",
    "`#### **** YOUR CODE HERE **** ####`\n",
    "\n",
    "There is also one place where you are asked to enter explanatory text (in Task 3b).\n",
    "Full instructions as to what is expected should be found above all the relevant cells.\n",
    "\n",
    "**Please submit your completed workbook using Moodle**. \n",
    "The workbook you submit must be an `.ipynb` file, which is saved into the \n",
    "directory you're running Jupyter; alternatively you can download it from the menu above using \n",
    "`File -> Download`. Remember to save your work regularly\n",
    "(\"Save Notebook\" in the File menu, the icon of a floppy disk, or Ctrl-S);\n",
    "the version you submit should have all code blocks showing the results (if any) \n",
    "of execution below them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important!\n",
    "\n",
    "**You should take care to avoid any suggestion of plagiarism in your submission.**\n",
    "There is helpful information on \"Academic Integrity\" and, specifically, how to avoid plagiarism\n",
    "on the University website: https://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "For this coursework, **you may use library code from specific sources only**: `numpy`, `scipy` and `matplotlib`. If you wish to use any alternative libraries, you are welcome to request this via the *Moodle* discussion forum.\n",
    "\n",
    "Note that **the use of Generative AI is not permitted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Coursework Tasks\n",
    "\n",
    "This coursework focuses on implementing some of the requisite calculations and manipulations \n",
    "for applying Bayesian linear regression models as covered in Lectures 3&ndash;6.\n",
    "\n",
    "Exercises include:\n",
    "\n",
    "- computing the posterior distribution,\n",
    "- computing the marginal likelihood,\n",
    "- evaluating and choosing regularisation parameters,\n",
    "- obtaining posterior mean models,\n",
    "- calculation and exploitation of error-bars (the predictive standard deviation),\n",
    "- ... some presentation of results.\n",
    "\n",
    "The data is synthetic, derived from a \"Gaussian\" radial basis function (RBF) model. The model we will attempt\n",
    "to fit is similarly a linearly-weighted set of \"Gaussian\" (RBF) basis functions. Our model, however,\n",
    "will incorporate basis functions of different radius (width) than the generator. (In the second coursework\n",
    "we will attempt to re-estimate this radius parameter.)\n",
    " \n",
    "The training data has an important feature: *there are no observations from one\n",
    "particular region of the data space*. \n",
    "One of the aims of the exercise (Task 3) is to see how this aspect \n",
    "impacts on the uncertainty of the model predictions.\n",
    "\n",
    "There are three principal tasks, with varying marks. In summary here:\n",
    "\n",
    "**Task 1:** Fit various Gaussian RBF-based linear models to the training data\n",
    "using penalised least-squares (PLS), and visualise the results. (5 marks)\n",
    "\n",
    "**Task 2:** Replicate the example \"Occam's Razor\" slide from Lecture 4\n",
    "using the model and data here &mdash; that is, compute the train, validation and test set errors,\n",
    "plus the marginal likelihood (the red dashed curve in the example slide), \n",
    "over a range of $\\lambda$ (or $\\alpha$) values. Identify and plot the best posterior mean model. (18 marks)\n",
    "\n",
    "**Task 3:** Visualise the error-bars (predictive variance) alongside the best posterior mean model.\n",
    "Explore how those error-bars might be usefully exploited in a real-world system where there is risk and reward.\n",
    "(7 marks)\n",
    "\n",
    "### \"Advanced\" Marks\n",
    "**Please note!** Across assignments, there may be a small number of marks which are designated \"advanced\" marks.\n",
    "These apply to sub-tasks which might be a little more technically demanding, and can optionally be skipped without interrupting the \"flow\" of an assignment.\n",
    "In this particular coursework there are two \"advanced\" marks, and even if you ignore those questions, you can still obtain over 93% of the marks overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Support code\n",
    "\n",
    "To get started, the code cell below imports the requisite standard Python modules.\n",
    "**You may need to add some of your own `import` statements here too, although note the\n",
    "restrictions specified earlier regarding library code use.**\n",
    "\n",
    "In addition, there is a setup module `setup_cw1` specific to this lab. This module contains:\n",
    "\n",
    "- the class `DataGenerator` to synthesise all the data sets needed,\n",
    "- the class `RBFGenerator` to create the necessary Gaussian \"RBF\" basis matrices for \n",
    "varying data sets,\n",
    "- the function `error_rms` to simply calculate errors for a given target values \n",
    "$y$ and corresponding model output $f$,\n",
    "- a function `plot_regression` to consistently plot data sets and curves etc,\n",
    "- a couple of helper functions to optionally enable neat tabulation of outputs: `tabulate_locals` and\n",
    "`tabulate_neatly`.\n",
    "\n",
    "*You don't need to use the plotting and tabulation code* if you don't want to!\n",
    "\n",
    "We also set some \"constants\" below: data set sizes and the generative noise standard deviation, \n",
    "which we fix at $\\sigma=0.15$ for the entire exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.006695Z",
     "start_time": "2025-02-11T21:55:46.412908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard modules\n",
    "import numpy as np\n",
    "\n",
    "# Lab-specific support module\n",
    "import setup_cw1 as setup\n",
    "\n",
    "# Add your own imports here\n",
    "\n",
    "\n",
    "# Data specification\n",
    "N_train_full = 50  # Before any data is \"masked out\"\n",
    "N_val_full = N_train_full\n",
    "N_test = 1000\n",
    "#\n",
    "sigma = 0.15\n",
    "sigma_2 = sigma ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "We synthesise three data sets:\n",
    "- training set of size $N_{train}=32$ (originally 50, but 18 points are \"masked out\") with added Gaussian random noise\n",
    "with standard deviation $\\sigma=0.15$,\n",
    "- validation set of size $N_{val}=32$, again with added noise $\\sigma=0.15$,\n",
    "- test set of size $N_{test}=1000$ with **no noise** and **covers the full data space**.\n",
    "\n",
    "A feature of the test set is that *it will include data from the region where there is no training data*.\n",
    "\n",
    "**Note:** we assume for now that $\\sigma$ is known and fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.018770Z",
     "start_time": "2025-02-11T21:55:47.013852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data - create generator instance, and synthesise 3 sets from\n",
    "# an RBF model using 10 basis functions and no bias\n",
    "#\n",
    "radius_gen = 1.00  # Generative basis radius or width\n",
    "train_mask = [3, 6]  # The range where there will be no data\n",
    "generator = setup.DataGenerator(r=radius_gen, noise=sigma, mask=train_mask)\n",
    "#\n",
    "x_train, y_train = generator.get_data('TRAIN', N_train_full, mask_out=True)\n",
    "x_val, y_val = generator.get_data('VALIDATION', N_val_full, mask_out=True)\n",
    "x_test, y_test = generator.get_data('TEST', N_test, mask_out=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Basis for Linear Model\n",
    "For our linear model, we use as many functions as data points (a \"complete\" basis), \n",
    "comprising $N-1$ equally-spaced Gaussian functions of the form:\n",
    " \n",
    "$$\\phi_m(x; c_m, r) = \\exp\\{-(x-c_m)^2/r^2\\}.$$\n",
    " \n",
    "plus a fixed \"bias\" or \"offset\". (The synthesised data was based on a different set of basis functions and did *not* use a bias.)\n",
    "\n",
    "The model basis function 'width' (length scale) parameter $r$ is set to 0.75. (This is narrower than the width\n",
    "used to generate the data.)\n",
    "\n",
    "If we call `evaluate` on the basis generator, \n",
    "we get a $N\\times{}M$ matrix $\\mathbf{\\Phi}$ returned, where each column / row contains \n",
    "the output of each basis function on each data point respectively: that is, \n",
    "$\\mathbf{\\Phi}_{nm} = \\phi_m(x_n)$. The use of a bias means the first column contains \n",
    "simply a fixed value of one. \n",
    "\n",
    "For the training set, this matrix $\\mathbf{\\Phi}$ will be $32 \\times 32$,\n",
    "whereas for the test set it will be $1000 \\times 32$.\n",
    "\n",
    "For illustration, the data and the underlying \"ground-truth\" are shown below, \n",
    "with basis functions overlaid. Some local variables are also tabulated, just to summarise the key variables.\n",
    "(This can also be a useful \"sanity check\" in longer notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.279270Z",
     "start_time": "2025-02-11T21:55:47.101174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basis - create generator instance and compute the basis matrices for all 3 data sets\n",
    "# Note that because we use a \"bias\" function, we need N-1 Gaussians to make the\n",
    "# basis \"complete\" (i.e. for M=N)\n",
    "#\n",
    "N_train = len(x_train)\n",
    "N_val = len(x_val)\n",
    "radius_model = radius_gen * 0.75\n",
    "centres = np.linspace(generator.x_min, generator.x_max, N_train - 1)\n",
    "basis = setup.RBFGenerator(centres, width=radius_model, bias=True)\n",
    "M = basis.M\n",
    "#\n",
    "PHI_train = basis.evaluate(x_train)\n",
    "PHI_val = basis.evaluate(x_val)\n",
    "PHI_test = basis.evaluate(x_test)\n",
    "#\n",
    "interest = [\"N_train\", \"N_val\", \"N_test\", \"sigma\", \"sigma_2\",\n",
    "            \"M\", \"radius_gen\", \"radius_model\"]\n",
    "setup.tabulate_locals(locals(), interest)\n",
    "#\n",
    "setup.plot_regression(x_train, y_train, x_test, y_test, basis_test=PHI_test,\n",
    "                      title=\"Target Function, Training Data & Basis\", legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Fitting Penalised Least Squares Models\n",
    "**5 marks for this task in total.**\n",
    "\n",
    "## Task 1a\n",
    "*2 marks overall*\n",
    "\n",
    "#### Implement penalised least squares fitting (2 marks)\n",
    "\n",
    "Write a function `fit_pls` (its signature is defined in the cell below) to fit a linear model with basis matrix `PHI_train` to the training data `y_train` for a given value of regularisation parameter $\\lambda$. It should return the weight vector $\\mathbf{w}_{PLS}$ that minimises the penalised least squares error.\n",
    "\n",
    "You may find the functions `np.linalg.lstsq` and/or `np.linalg.inv` applicable.\n",
    "\n",
    "For maximum marks, the ideal solution will treat $\\lambda=0$ case differently to $\\lambda>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.295316Z",
     "start_time": "2025-02-11T21:55:47.293180Z"
    }
   },
   "outputs": [],
   "source": [
    "## FIT_PLS\n",
    "##\n",
    "def fit_pls(PHI, y, lam):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b\n",
    "*3 marks overall*\n",
    "\n",
    "#### Implement a prediction assessor (1 mark)\n",
    "\n",
    "Write a function `assess_predictor` that does the following:\n",
    "\n",
    "- fits the model to the data for a given value of lambda using `fit_pls`\n",
    "- computes the fitted model's predictions `f_test` on the *test set* (you will need to multiply the\n",
    "  relevant $\\mathbf{\\Phi}$ by $\\mathbf{w}_{PLS}$)\n",
    "- plots the training data, target function (test data) and the predictions (using `plot_regression`)\n",
    "- returns the RMS training and test errors\n",
    "\n",
    "#### Visualise fits for three values of λ (1 mark)\n",
    "\n",
    "Using the function you have just written, add a few lines of code in the second cell below\n",
    "to plot three graphs for values of $\\lambda$ in \\[ 1e-9, 0.05, 25\\]. In each graph (appropriately labelled by $\\lambda$),\n",
    "your `assess_predictor` function should show:\n",
    "- the training data (`x_train` and `y_train` as points)\n",
    "- the underlying generating function (by plotting `x_test` and `y_test` as a line)\n",
    "- your fitted function `f_test` at points `x_test` (as a line)\n",
    "\n",
    "#### Tabulate PLS errors (1 mark)\n",
    "\n",
    "Output a table, showing the RMS error for both train and test sets for all three values of $\\lambda$.\n",
    "\n",
    "The supplied function `plot_regression` should do nearly all the plotting work for you, but please add an informative title.\n",
    "(You do not need to show the basis.)\n",
    "\n",
    "You should find that the first $\\lambda$ value over-fits, the second is \"about right\" \n",
    "(but only where there is data!) and the third under-fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.345966Z",
     "start_time": "2025-02-11T21:55:47.342934Z"
    }
   },
   "outputs": [],
   "source": [
    "## FITTING & GRAPHING FUNCTION\n",
    "#  \n",
    "def assess_predictor(model_rbf, lamb, x__train, y__train, x__test, y__test):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n",
    "    return error_train, error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.780756Z",
     "start_time": "2025-02-11T21:55:47.400175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add code to plot the requested graphs for lambda_value=1e-9, 0.05 and 25 respectively,\n",
    "# while simultaneously compiling a table of errors\n",
    "#\n",
    "#### **** YOUR CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.808282Z",
     "start_time": "2025-02-11T21:55:47.804286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output your table here\n",
    "#\n",
    "#### **** YOUR CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Bayesian Modelling & the Marginal Likelihood\n",
    "**18 marks for this task in total.**\n",
    "\n",
    "## Task 2a\n",
    "*3 marks overall*\n",
    "\n",
    "#### Compute the posterior distribution (3 marks)\n",
    "\n",
    "Write a function `compute_posterior` to compute the posterior mean $\\mathbf{\\mu}$ and \n",
    "covariance $\\mathbf{\\Sigma}$ for the Bayesian linear regression model with \n",
    "basis matrix $\\mathbf{\\Phi}$ and with hyperparameters $\\alpha$ and \n",
    "$\\sigma^2$.\n",
    "\n",
    "Verify the consistency of your posterior code with `fit_pls` by comparing \n",
    "the outputs `w` and `Mu` (they should be the same), remembering to take account of the fact \n",
    "that $\\alpha \\equiv \\lambda/\\sigma^2$.\n",
    "\n",
    "As well as defining your `compute_posterior` function below, \n",
    "append a few lines of code underneath \n",
    "which show your consistency check for $\\lambda=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:47.906419Z",
     "start_time": "2025-02-11T21:55:47.902480Z"
    }
   },
   "outputs": [],
   "source": [
    "## POSTERIOR\n",
    "##\n",
    "def compute_posterior(PHI, y, alpha, sigma_squared):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n",
    "\n",
    "# check consistency\n",
    "#\n",
    "#### **** YOUR CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b\n",
    "*3 marks overall*\n",
    "\n",
    "#### Compute the marginal likelihood (2 marks)\n",
    "\n",
    "Write a function `compute_log_marginal` to compute the *logarithm* of\n",
    "the marginal likelihood for the Bayesian linear regression model with basis matrix \n",
    "$\\mathbf{\\Phi}$ and with hyperparameters $\\alpha$ and $\\sigma^2$.\n",
    "\n",
    "The necessary equation(s) may be found in the slides / notes corresponding to Lecture 4\n",
    "(\"Marginalisation & Occam's Razor\"):\n",
    " \n",
    "$$p(\\mathbf{y}|\\alpha, \\sigma^2) = \n",
    "(2 \\pi)^{-\\frac{N}{2}} |\\sigma^2 \\mathbf{I} + \n",
    "\\alpha^{-1} \\mathbf{\\Phi}\\mathbf{\\Phi}^{\\top}|^{-\\frac{1}{2}} \n",
    "\\exp\\{ -\\frac{1}{2} \\mathbf{y}^{\\top} \n",
    "(\\sigma^2 \\mathbf{I} + \\alpha^{-1} \\mathbf{\\Phi}\\mathbf{\\Phi}^{\\top})^{-1} \\mathbf{y}  \\}.$$\n",
    "\n",
    "**Important:** for numerical reasons, when computing the logarithm of a probability density,\n",
    "always compute the logarithmic form directly (whether in terms of your own Python code or\n",
    "via a library such as `stats.multivariate_normal.logpdf`).\n",
    "**Never** compute the pdf and then call `np.log`!\n",
    "\n",
    "In practice, you should not have numerical issues using \n",
    "`stats.multivariate_normal.logpdf` (this can happen if you experiment \n",
    "with larger data sets, in which case you may wish to look at the `allow_singular` argument.)\n",
    "You can also code up your own calculation \"manually\" without use of library code (in two different ways).\n",
    "\n",
    "\n",
    "#### Woodbury Identity (advanced) (1 mark)\n",
    "\n",
    "**Advanced**: As an alternative to `stats.multivariate_normal.logpdf`, there is a more\n",
    "robust way of calculating the marginal likelihood directly, which you may wish to explore\n",
    "if you are confident with linear algebra and matrix identities.\n",
    "\n",
    "**Note:** this advanced mark is\n",
    "in addition to that above &mdash; you should also provide a \"standard\" implementation of `compute_log_marginal`\n",
    "to obtain full marks for Task 2b.\n",
    "\n",
    "The \"straightforward\" way to calculate the marginal likelihood is to compute \n",
    "the NxN covariance matrix and then call the `logpdf` function.\n",
    "As a result, this will imply an expensive, and potentially numerically troublesome, \n",
    "inversion of an N x N matrix (inside `logpdf`). This works OK, but in general we can do better.\n",
    "\n",
    "To gain the extra advanced mark, you would need to re-phrase the straightforward way\n",
    "in terms of the inversion, and determinant, of an M x M matrix. \n",
    "Since matrix inversion scales cubically with the matrix size (M or N),\n",
    "this is computationally a lot cheaper in the usual case where N >> M,\n",
    "and more importantly, numerically much more robust.\n",
    "\n",
    "Slightly confusingly in this exercise here, N = M, so it doesn't appear \n",
    "to make any difference. However, in more general modelling situations, \n",
    "M would be a lot smaller than N, so it would be an important thing to do. \n",
    "In terms of re-phrasing the inversion as M x M, you may wish to look at \n",
    "the \"Woodbury Identity\", which crops up regularly in data science.\n",
    "There is an equivalent identity for determinants too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.002759Z",
     "start_time": "2025-02-11T21:55:48.000338Z"
    }
   },
   "outputs": [],
   "source": [
    "## MARGINAL LIKELIHOOD\n",
    "##\n",
    "def compute_log_marginal(PHI, y, alph, s2):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MARGINAL LIKELIHOOD (ADVANCED VERSION - OPTIONAL)\n",
    "##\n",
    "def compute_log_marginal_advanced(PHI, y, alph, s2):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c\n",
    "*9 marks overall*\n",
    "\n",
    "In the cells below, write some code to produce a similar figure to that below (taken from the lecture) for the\n",
    "Gaussian RBF basis and the data set(s) defined above.\n",
    "\n",
    "<img src=\"slide15.png\" alt=\"Slide 15 from Lecture 5\"\n",
    " style=\"width: 512px;margin-left:16mm;\"/>\n",
    "\n",
    "In more detail, over a range of $\\lambda$ (or equivalent $\\alpha=\\lambda/\\sigma^2$) values, \n",
    "you should do the following:\n",
    "\n",
    "#### Compute the relevant errors (1 mark)\n",
    "\n",
    "Compute the train, validation and test set errors for\n",
    "the penalised least-squares model (use your `fit_pls` to fit it, and the supplied `error_rms`\n",
    "in 'setup_cw1' to calculate the error).\n",
    "\n",
    "#### Compute the (negative log) marginal likelihood (2 marks)\n",
    "\n",
    "Also compute the *negative* log marginal likelihood using the function just written,\n",
    "for the training data set *and a data set which combines both the training and validation data*.\n",
    "\n",
    "#### Plot the curves (3 marks)\n",
    "\n",
    "Plot **all these curves on the same graph**, noting that the vertical axis for\n",
    "the errors is different for that of the negative log marginal likelihood (see hint below),\n",
    "and the horizontal axis, while shared, requires careful translation between λ and α.\n",
    "\n",
    "Credit will be given for a clearly rendered graph, with sensible axis scaling.\n",
    "\n",
    "Note that it's *not* essential to plot the vertical lines which connect the curves.\n",
    "\n",
    "#### Find minimum points and tabulate test errors (2 marks)\n",
    "\n",
    "In addition to plotting the graph, add code to compute,\n",
    "and output (using `print` or `tabulate_neatly`), the *test error* corresponding to\n",
    "the minimum point on:\n",
    "- the test error curve itself,\n",
    "- the validation curve,\n",
    "- the negative marginal likelihood curve (on the training data),\n",
    "- the negative marginal likelihood curve (on combined training and validation data).\n",
    "\n",
    "#### Find the best α value (1 mark)\n",
    "\n",
    "Calculate a value of `best_alpha`, which is the value of $\\alpha$ that minimises\n",
    "the negative marginal likelihood curve (on *combined* training and validation data).\n",
    "\n",
    "### Notes\n",
    "\n",
    "For easiest interpretation, please define your $\\lambda$ range \n",
    "logarithmically (**base 10**) and **fix the range between -6 and 4**.\n",
    "\n",
    "First use `np.linspace(begin,end,steps)` to create equally spaced values `v`, \n",
    "then specify `lam = 10**v`. Please fix the `steps` value as 100.\n",
    "\n",
    "You should find that your graph has a similar qualitative form to the above figure, \n",
    "although there will be one extra curve and the test error curve will be higher above the\n",
    "training/validation curves.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- You can create a new $y$-axis on the same plot, which shares the $x$-axis, \n",
    "using `plt.gca().twinx()`.\n",
    "- Remember that $\\alpha=\\lambda/\\sigma^2$. If you don't rescale appropriately, \n",
    "the marginal likelihood curve will not be correctly aligned with the error curves \n",
    "and your minimum point will be incorrect! 😟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.093346Z",
     "start_time": "2025-02-11T21:55:48.090195Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#### **** YOUR CODE HERE **** ####\n",
    "#\n",
    "\n",
    "# Calculate the error for the train, test and validation sets and\n",
    "# the negative log marginal-likelihood on the train set and combined\n",
    "# train and validation sets ... all for a range of lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.456020Z",
     "start_time": "2025-02-11T21:55:48.153987Z"
    }
   },
   "outputs": [],
   "source": [
    "# PLEASE PLOT ALL THE CURVES IN THE SAME FIGURE\n",
    "\n",
    "# Print out (or tabulate) the test error corresponds to the minimum point on (1) the test error curve itself,\n",
    "# (2) the validation curve, and (3, 4) the negative marginal-likelihood curves.\n",
    "#\n",
    "#### **** YOUR PLOTTING CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d\n",
    "*3 marks overall*\n",
    "\n",
    "#### Evaluate the posterior mean predictor (2 marks)\n",
    "\n",
    "Using the best value of $\\alpha$ according to the marginal likelihood as found in Task 2c \n",
    "above, call `compute_posterior` to find the posterior mean weight vector $\\mu$ and \n",
    "use this to compute the posterior mean predictor at all the test points \n",
    "`x_test`. \n",
    "\n",
    "#### Visualise the predictor (1 mark)\n",
    "\n",
    "Then, similar to Task 1b, using the `plot_regression` function, plot on the same axes:\n",
    "1. the training data (`x_train` and `y_train` as points),\n",
    "2. the underlying generating function (by plotting `x_test` and `y_test` as a line),\n",
    "3. the posterior mean predictor function you just calculated.\n",
    "\n",
    "You should find that the predictor is a pretty good fit to the underlying \n",
    "generating function *where there was data in the training set*. \n",
    "We can't expect the model to make good predictions in the \"gap\" \n",
    "where it has not seen any training data (unless it gets lucky!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.481885Z",
     "start_time": "2025-02-11T21:55:48.480152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code to calculate posterior mean weights and corresponding predictor\n",
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.618438Z",
     "start_time": "2025-02-11T21:55:48.530156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise the predictor\n",
    "#\n",
    "#### **** YOUR PLOTTING CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Predictive Variance and Error Bars <a name=\"task3\"></a>\n",
    "**7 marks for this task in total.**\n",
    "\n",
    "## Task 3a\n",
    "*2 marks overall*\n",
    "\n",
    "#### Compute the predictive variance and then plot the \"error-bars\" (2 marks)\n",
    "\n",
    "Remember that a \"prediction\" of $y_\\ast$ at a particular $x_\\ast$ in the Bayesian sense is not a single value, but actually a\n",
    "*distribution* over possible values that a new datum $y_\\ast$ (sampled at $x_\\ast$) might take,\n",
    "based on the model definition and the parameter settings. That\n",
    "distribution encapsulates the inevitable uncertainty in prediction, taking account of both the data noise\n",
    "variance (which can't be avoided) and the uncertainty in estimating the weights (which will decrease with more training data).\n",
    "Given our model choices, we have a Gaussian predictive distribution at each $x$,\n",
    "and the graph you plotted above (in Task 2d) shows just the *mean* of the distribution at each point.\n",
    "\n",
    "Repeat the graph above (Task 2d), for the same \"best\" predictor, but this time also\n",
    "compute the predictive variance at each point in `x_test`, and use these values to overlay \n",
    "\"error bars\" on the plot.\n",
    "\n",
    "### Notes\n",
    "\n",
    "The predictive variance can be computed from the posterior covariance matrix\n",
    "$\\mathbf{\\Sigma}$ in conjunction with the `PHI_test` basis matrix, along with the noise variance $\\sigma^2$.\n",
    "To plot the \"error bars\", you will need to compute the predictive *standard deviation* at each test point &mdash;\n",
    "in other words, the square-root of the predictive variance.\n",
    "\n",
    "- By \"error bars\" we usually mean some chosen multiple of the predictive standard deviation; please\n",
    "opt for +/- **one standard deviation** here,\n",
    "- You'll need to compute this for every $x$ &mdash; you can do this with a loop, but there is also\n",
    "an efficient way to do it with matrix and vector operations\n",
    "- To compute the predictive variance at `x_test`, you'll need the matrix `PHI_test`,\n",
    "- The matplotlib function `plt.fill_between` is a handy utility for plotting error bars, \n",
    "but you may also wish to look at the functionality in `plot_regression`,\n",
    "- If your calculations are correct, you should expect to see much larger error bars in the gap\n",
    "where there is no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.642313Z",
     "start_time": "2025-02-11T21:55:48.640539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the predictive variance\n",
    "#\n",
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.782161Z",
     "start_time": "2025-02-11T21:55:48.687671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the error bars\n",
    "#\n",
    "#### **** YOUR PLOTTING CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b\n",
    "*5 marks overall*\n",
    "\n",
    "This is a more open-ended question, with an opportunity to think a little creatively.\n",
    "\n",
    "Imagine you had built the above model for inclusion within some real-world system where \n",
    "it was desired to make accurate predictions at the 1,000 test points. For this exercise, you\n",
    "can assume there is no data noise when deployed, and the only predictive uncertainty is due\n",
    "to the weights of the trained model.\n",
    "\n",
    "The \"real-world system\" is such that there are costs and rewards associated with each prediction:\n",
    "\n",
    "- for each absolute error of `threshold=0.15` or less, the model is rewarded with £10\n",
    "- for each absolute error of greater than `threshold`, there is a cost of £20\n",
    "\n",
    "**Key feature of the \"system\":** *You may decline to make a prediction for a given data point*, based on the knowledge of $x$.\n",
    "If you do decline to make a prediction, there is no cost (or reward).\n",
    "\n",
    "### Devise an Algorithm Which Chooses When to Predict (and When to Decline)\n",
    "*Without knowledge of the test set*, devise an algorithm that aims to maximise your *expected* earnings.\n",
    "That is, an algorithm that decides, given an arbitrary $x$, whether to predict $y$ or whether to decline.\n",
    "This can only be an approximate maximisation (remember, \"all models are wrong\"), but, given the model,\n",
    "there is a principled way to make a decision whether to predict or decline, given the cost and reward\n",
    "specification.\n",
    "\n",
    "Specifically, where indicated in the below cells (and detailed further below), do the following four things:\n",
    "\n",
    "1. Explain your approach\n",
    "2. Implement your algorithm\n",
    "3. Evaluate it and tabulate results\n",
    "4. **Advanced:** Visualise its decision-making\n",
    "\n",
    "#### Explain your algorithm (2 marks)\n",
    "\n",
    "Write a short explanation of your approach in the \"Explanation of Approach\" markdown cell.\n",
    "\n",
    "#### Implement your algorithm (1 mark)\n",
    "\n",
    "Provide the necessary decision code for your algorithm in the first code cell below.\n",
    "\n",
    "#### Evaluate your approach and tabulate some statistics (1 mark)\n",
    "\n",
    "In the second code cell below, apply your algorithm to the 1,000 (noiseless) test points `x_test` and tabulate:\n",
    "\n",
    "- The number of predictions (out of 1,000) made\n",
    "- The number that were rewarded for being within threshold\n",
    "- The number that were charged for being outside the threshold\n",
    "- The percentage of predictions made that were rewarded\n",
    "- Your total earnings\n",
    "\n",
    "You should be able to make as much as £5,000 (or thereabouts, without cheating!).\n",
    "If your algorithm is over-confident and makes too many predictions, you may do poorly due to incurring many costs. Alternatively, if your algorithm is too conservative and declines to predict too often, you may do poorly due to not earning enough rewards.\n",
    "\n",
    "#### Visualise the operation of your algorithm (advanced) (1 mark)\n",
    "\n",
    "**Advanced:** Plot a graph which illustrates where your algorithm chooses to predict and where it chooses to decline.\n",
    "More specifically, you should show:\n",
    "- the target function (test points `x_test` and `y_test` as a line)\n",
    "- your predictor (posterior mean)\n",
    "- the error-bars, but only where the algorithm chooses to predict, and in one particular colour of your choosing\n",
    "- the error-bars, this time only where the algorithm chooses to decline, and in a different colour of your choosing\n",
    "- a helpful legend\n",
    "- Optionally: the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Approach (*Your Answer Here*)\n",
    "\n",
    "*Explain your approach and algorithm here in this cell (and delete this text)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.793623Z",
     "start_time": "2025-02-11T21:55:48.791894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement your algorithm here\n",
    "#\n",
    "# Use these fixed values\n",
    "threshold = 0.15\n",
    "reward_under = 10\n",
    "cost_above = 20\n",
    "\n",
    "#### **** YOUR ALGORITHM CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:55:48.943223Z",
     "start_time": "2025-02-11T21:55:48.839170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply your algorithm to the test set, and tabulate the relevant statistics\n",
    "\n",
    "#### **** YOUR TABULATION CODE HERE **** ####\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise your algorithm's decision-making (advanced / optional)\n",
    "\n",
    "#### **** YOUR GRAPHING CODE HERE **** ####\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
