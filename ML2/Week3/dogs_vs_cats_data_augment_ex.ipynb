{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5YBJoDlGHCG"
      },
      "source": [
        "# Experiment on Data Augmentation\n",
        "In the previous notebook, you have done image classifications on the small **Dogs vs Cats** dataset. As we only used a small subset of the dataset containing 2,000 images for training, 1,000 for validation and 1,000 for testing, we observed significate overfitting problem.\n",
        "\n",
        "Now, in this example, we will address the overfitting problem with **data augmentation**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjbeEiUHMbt"
      },
      "source": [
        "## Setting-Up 1: Mount Google Drive to the notebook\n",
        "You can easily load data from Google Drive by mounting it to the notebook with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIL6lGwXFz-n",
        "outputId": "578f1c7f-7cf4-4570-b41f-31dd556fd738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkb1qsilHaJQ"
      },
      "source": [
        "## Setting-Up 2: One click to enable FREE GPU\n",
        "Don't forget to enable GPU in your Colab notebook before training your model.\n",
        "\n",
        "In Google Colab, it is very easy to do so.\n",
        "\n",
        "From task bar, click: Runtime ⇨ Change runtime type\n",
        "\n",
        "Choose 'GPU' in the Hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDtA7_deJwSd"
      },
      "source": [
        "## Dataset\n",
        "Before you start this notebook, make sure the small dataset `dogs-vs-cats-small/`, which was generated in `dogs_vs_cats.ipynb`, is saved in your Google Drive. We will not repeat the \"Downloading data ==> Creating small dataset\" process in this notebook.\n",
        "\n",
        "For efficient deep learning training in Google Colab, it is strongly recommended copying datasets from Google Drive to Colab’s local storage (`/content`) before training.\n",
        "The benefit of using local directory is:\n",
        "- Faster data access → Avoids slow I/O from Google Drive API.\n",
        "- Better GPU performance → Reduces bottlenecks in loading images/batches.\n",
        "- More stable training → Prevents disconnections from Google Drive.\n",
        "You may use first copy datasets to the local directory with command `rsync`(recommended) or `cp`(slower):\n",
        "\n",
        "`!rsync -avh \"/content/drive/MyDrive/dataset/\" \"/content/dataset/\"`\n",
        "\n",
        "Please note: The `/content` directory is temporary and will be deleted when the session resets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbqA6Qs6JMOK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Colab's local storage to store the dataset for faster training\n",
        "local_dataset_dir = \"/content/dogs-vs-cats-small\"\n",
        "if not os.path.exists(local_dataset_dir): # always check if the directory exists\n",
        "  # Copy the dataset (preserves all subfolders and files)\n",
        "  print(\"Copy dataset to Colab's local storage...\")\n",
        "  !rsync -avh \"/content/drive/MyDrive/Colab Notebooks/data/dogs-vs-cats-small/\" \"/content/dogs-vs-cats-small/\"#change the first directory to the dataset path in your drive\n",
        "  print(\"Copy done.\")\n",
        "\n",
        "train_dir = os.path.join(local_dataset_dir,'train')\n",
        "val_dir   = os.path.join(local_dataset_dir,'val')\n",
        "test_dir  = os.path.join(local_dataset_dir,'test')\n",
        "if not os.path.exists(train_dir):\n",
        "  print(train_dir +' does not exist.')\n",
        "if not os.path.exists(val_dir):\n",
        "  print(val_dir +' does not exist.')\n",
        "if not os.path.exists(test_dir):\n",
        "  print(test_dir +' does not exist.')\n",
        "\n",
        "n_train_per_class = 1000\n",
        "n_val_per_class = 500\n",
        "n_test_per_class = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyB5lVnJ56nY"
      },
      "source": [
        "## STEP 1: Data preprocessing\n",
        "Now we need do some pre-processing before feeding the data into the network.\n",
        "\n",
        "Roughly, the preprocessing consists of following steps.\n",
        "1. Read the image files, decode them to RGB grids of pixels\n",
        "2. Rescale the pixel values (integers between 0 and 255) to the [0,1] interval, to enhance training stability of neural networks.\n",
        "3. Apply data augmentation.\n",
        "\n",
        "There are two ways to apply augmentation to training images using the random transforms of `layers`.\n",
        "\n",
        "**Option 1**: Make the preprocessing layers part of your model. It directly add the augmentation layer into the model structure as the first layer.\n",
        "\n",
        "**Option 2**: Apply the preprocessing layers to your dataset. It is to apply the data augmentation to the entire train set using `Dataset.map`.\n",
        "\n",
        "You may reference the follow link for both options: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "\n",
        "Note: In either option, no data augmentation is applied for either test or validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4wmRDGu6ApM"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Add your code here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A01oaeKTh4"
      },
      "source": [
        "## STEP 1: Build the CNN network\n",
        "Use the same small convnet in `dogs_vs_cats.ipynb`. **Don't add any other regularization** techniques (such as dropout), as you will know any performance changes in this experiment is purely because of data augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaJm9TskKkqc"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Add your code here\n",
        "#\n",
        "model.summary()\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True, dpi=100) # visualize the CNN artitecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF2lOpGdKpQa"
      },
      "source": [
        "## STEP 2: Compile the model\n",
        "The typical loss function for a binary classification problem is the binary cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svCSB3EdKuuU"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Add your code here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kDVKVfaLjV"
      },
      "source": [
        "## STEP 4: Train the model and draw learning curves\n",
        "Let's train the model. You may need more epochs in this training, say `epochs=120`.\n",
        "\n",
        "It is a good practice to always save your models after training with `model.save_model(model_folder+'/dogs_cats_small_data_augment.keras') `.\n",
        "\n",
        "After training, also plot the loss and accuracy of the model over the training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLSqHkfamWp"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Add your code here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkD0QyFebFg2"
      },
      "source": [
        "**Q:** According to your learning curve, is the overfitting problem solved with data augmentation? Is there any accuracy improvement?  \n",
        "\n",
        "**Optional:** After successfully completing data augmentation, you can fine-tune the network’s parameters or incorporate additional regularization techniques, such as dropout, early stopping, weight regularization, or batch normalization, to further improve accuracy.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}