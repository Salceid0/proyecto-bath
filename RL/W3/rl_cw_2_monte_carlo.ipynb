{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a1c8fb41527c5d768d3632cdc24e8df",
     "grade": false,
     "grade_id": "cell-b0b3b6b4f891b031",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM30359 Reinforcement Learning\n",
    "\n",
    "## Monte Carlo Methods\n",
    "In this lab session, we will be guiding you through implementing one of the Monte Carlo control algorithms discussed in this week's lectures and applying it to solve a simple sequential decision problem.\n",
    "\n",
    "Specifically, you will implement the On-Policy First-Visit Monte-Carlo Control algorithm and use it to train an agent to drive a car around a simple simulated racetrack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47878012611fb5c419e107ca45b63a02",
     "grade": false,
     "grade_id": "cell-e86e35a4b405ff32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Racetrack Environment\n",
    "We have implemented a custom environment called \"Racetrack\" for you to use during this lab. It is inspired by the environment described in the course textbook (Reinforcement Learning, Sutton & Barto, 2018, Exercise 5.12), but is not exactly the same.\n",
    "\n",
    "### Environment Description\n",
    "Consider driving a race car around a turn on a racetrack. In order to complete the race as quickly as possible, you would want to drive as fast as you can but, to avoid running off the track, you must slow down while turning.\n",
    "\n",
    "In our simplified racetrack environment, the agent is at one of a discrete set of grid positions. The agent also has a discrete speed in two directions, $x$ and $y$. So the state is represented as follows:\n",
    "$$(\\text{position}_y, \\text{position}_x, \\text{velocity}_y, \\text{velocity}_x)$$\n",
    "\n",
    "The agent collects a reward of -1 at each time step, an additional -10 for leaving the track (i.e., ending up on a black grid square in the figure below), and an additional +10 for reaching the finish line (any of the red grid squares). The agent starts each episode on a randomly selected grid-square on the starting line (green grid squares) with a speed of zero in both directions. At each time step, the agent can change its speed in both directions. Each speed can be changed by +1, -1 or 0, giving a total of nine actions. For example, the agent may increase its speed in the $x$ direction by -1 and its speed in the $y$ direction by +1. The agent's speed cannot be greater than +10 or less than -10 in either direction.\n",
    "\n",
    "<img src=\"images/track_big.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "The agent's next state is determined by its current grid square, its current speed in two directions, and the changes it  makes to its speed in the two directions. This environment is stochastic. When the agent tries to change its speed, no change occurs (in either direction) with probability 0.2. In other words, 20% of the time, the agent's action is ignored and the car's speed remains the same in both directions.\n",
    "\n",
    "If the agent leaves the track, it is returned to a random start grid-square and has its speed set to zero in both directions; the episode continues. An episode ends only when the agent transitions to a goal grid-square.\n",
    "\n",
    "\n",
    "\n",
    "### Environment Implementation\n",
    "We have implemented the above environment in the `racetrack_env.py` file, for you to use in this coursework. Please use this implementation instead of writing your own, and please do not modify the environment.\n",
    "\n",
    "We provide a `RacetrackEnv` class for your agents to interact with. The class has the following methods:\n",
    "- **`reset()`** - this method initialises the environment, chooses a random starting state, and returns it. This method should be called before the start of every episode.\n",
    "- **`step(action)`** - this method takes an integer action (more on this later), and executes one time-step in the environment. It returns a tuple containing the next state, the reward collected, and whether the next state is a terminal state.\n",
    "- **`render(sleep_time)`** - this method renders a matplotlib graph representing the environment. It takes an optional float parameter giving the number of seconds to display each time-step. This method is useful for testing and debugging, but should not be used during training since it is *very* slow. **Do not use this method in your final submission**.\n",
    "- **`get_actions()`** - a simple method that returns the available actions in the current state. Always returns a list containing integers in the range [0-8] (more on this later).\n",
    "\n",
    "In our code, states are represented as Python tuples - specifically a tuple of four integers. For example, if the agent is in a grid square with coordinates ($Y = 2$, $X = 3$), and is moving zero cells vertically and one cell horizontally per time-step, the state is represented as `(2, 3, 0, 1)`. Tuples of this kind will be returned by the `reset()` and `step(action)` methods. It is worth noting that tuples can be used to index certain Python data-structures, such as dictionaries.\n",
    "\n",
    "There are nine actions available to the agent in each state, as described above. However, to simplify your code, we have represented each of the nine actions as an integer in the range [0-8]. The table below shows the index of each action, along with the corresponding changes it will cause to the agent's speed in each direction.\n",
    "\n",
    "<img src=\"images/action_grid.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "For example, taking action 8 will increase the agent's speed in the $x$ direction, but decrease its speed in the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4714ee739adca912176564b3eb00229",
     "grade": false,
     "grade_id": "cell-30ac99abe97e62b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Racetrack Code Example\n",
    "Below, we go through a quick example of using the `RaceTrackEnv` class.\n",
    "\n",
    "First, we import the class, then create a `RaceTrackEnv` object called `env`. We then initialise the environment using the `reset()` method, and take a look at the initial state variable and the result of `plot()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac22a56ca4687400306302c35b75a91",
     "grade": false,
     "grade_id": "cell-77add459a6f282dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Initial State: (1, 3, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set random seed to make example reproducable.\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "from racetrack_env import RacetrackEnv\n",
    "\n",
    "# Instantiate environment object.\n",
    "env = RacetrackEnv()\n",
    "\n",
    "# Initialise/reset environment.\n",
    "state = env.reset()\n",
    "env.render()\n",
    "print(env.get_actions())\n",
    "print(\"Initial State: {}\".format(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf70e6e3c9fe761473c11366c91f40ff",
     "grade": false,
     "grade_id": "cell-b42bead8118e3c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, `reset()` has returned a valid initial state as a four-tuple. The function `plot()` uses the same colour-scheme as described above, but also includes a yellow grid-square to indicate the current position of the agent.\n",
    "\n",
    "Let's make the agent go upward by using `step(1)`, then inspect the result (recall that action `1` increments the agent's vertical speed while leaving the agent's horizontal speed unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "409bb221e1080a4a02e52db851d6dc86",
     "grade": false,
     "grade_id": "cell-8cb86c18bf331894",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: (1, 3, 0, 0), Reward: -11, Terminal: False\n"
     ]
    }
   ],
   "source": [
    "# Let us increase the agent's vertical speed (action 1).\n",
    "terminal = False\n",
    "next_state, reward, terminal = env.step(1)\n",
    "env.render()\n",
    "print(\"Next State: {}, Reward: {}, Terminal: {}\".format(next_state, reward, terminal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a68773faf856fac19dc863fe5e3b01f4",
     "grade": false,
     "grade_id": "cell-4f51e890424d0c2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see that the agent has moved one square upwards, and now has a positive vertical speed (indicated by the yellow arrow). Let's set up a loop to see what happens if we take the action a few more times, causing it to repeatedly leave the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50931f73b836a9941366a8e8b67805a2",
     "grade": false,
     "grade_id": "cell-ef8865037a9ebdeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 50\n",
    "for t in range(num_steps) :\n",
    "    next_state, reward, terminal = env.step(1)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "861fb36a2a639caac9b53043114d4cbf",
     "grade": false,
     "grade_id": "cell-a21a5643628d8354",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise: On-Policy MC Control\n",
    "\n",
    "### Instructions\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using On-Policy First-Visit MC Control, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 5.4 p.101).\n",
    "\n",
    "<img src=\"images/mc_control_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.15$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at **least** 20.\n",
    "\n",
    "Your implementation of a tabular **On-Policy First-Visit MC Control** agent should produce a list named `mc_rewards`. This list should contain one list for each agent that you train. Each sub-list should contain the undiscounted sum of rewards earned during each episode by the corresponding agent. <br />\n",
    "For example, if you train $20$ agents, your `mc_rewards` list will contain $20$ sub-lists, each containing $150$ integers. Below your implementation, we will use this list to plot a **learning curve**, which shows the average return earned by your agents each episode.\n",
    "\n",
    "### Getting Started\n",
    "If you are having trouble implementing the pseudocode shown above, here are some hints to help you get started:\n",
    "- Monte Carlo agents work by generating full episodes of experience, and then making updates based on the states visited, actions taken, and rewards earned during those episodes. As such, it would make sense for your code to consist of two parts:\n",
    "    - One which generates an episode of experience.\n",
    "    - One which takes an episode of experience and performs updates based on it.\n",
    "- Python has many datastructures that could be useful for implementing RL agents.\n",
    "    - Dictionaries can be useful for representing your agent's Q-table, as they allow mappings from keys (state-action pairs) to values (action values).\n",
    "    - Tuples can be useful for storing the experience your agent generates each time-step (state, action, reward, next-state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Q table\n",
    "q = {\n",
    "    (a, b, c, d): {k: 0 for k in range(1, 9)}\n",
    "    for a in range(1, 18)\n",
    "    for b in range(1, 14)\n",
    "    for c in range(-10, 11)\n",
    "    for d in range(-10, 11)\n",
    "}\n",
    "policy = {\n",
    "    (a, b, c, d): {k: 1/8 for k in range(1, 9)}\n",
    "    for a in range(1, 18)\n",
    "    for b in range(1, 14)\n",
    "    for c in range(-10, 11)\n",
    "    for d in range(-10, 11)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On policy first visit MC control for e-soft policies\n",
    "\n",
    "def generate_episode(env, policy, gamma=0.9, epsilon=0.15):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    terminal=False\n",
    "    env.render()\n",
    "    reward=0\n",
    "    while reward>-5:\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        episode.append((state, action, reward, terminal))\n",
    "        state = next_state\n",
    "    return episode\n",
    "def check_tuple(tuple,list):\n",
    "    for t in list:\n",
    "        if t[0]==tuple[0] and t[1]==tuple[1]:\n",
    "            return True\n",
    "    return False\n",
    "def monte_carlo_control(env,q,policy, num_episodes=150, gamma=0.9, epsilon=0.15):\n",
    "    returns = {\n",
    "    (a, b, c, d): {k: [] for k in range(1, 9)}\n",
    "    for a in range(1, 18)\n",
    "    for b in range(1, 14)\n",
    "    for c in range(-10, 11)\n",
    "    for d in range(-10, 11)\n",
    "                }\n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, gamma, epsilon)\n",
    "        g=0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward, terminal = episode[t]\n",
    "            if not terminal:\n",
    "                g = gamma * g + reward\n",
    "                if not check_tuple((state,action),episode[:t]):\n",
    "                    returns[state][action].append(g)\n",
    "                    q[state][action] = np.mean(returns[state][action])\n",
    "                    a_star= q[state][max(q[state], key=q[state].get)]\n",
    "                    for action in q[state]:\n",
    "                        if action!=a_star:\n",
    "                            policy[state][action]=epsilon/len(q[state])\n",
    "                        else:\n",
    "                            policy[state][action]=1-epsilon+epsilon/len(q[state])\n",
    "    return q,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36mgenerate_episode\u001b[1;34m(env, policy, gamma, epsilon)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminal:\n\u001b[0;32m      9\u001b[0m     random_number \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n\u001b[0;32m     12\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(policy[state], key\u001b[38;5;241m=\u001b[39mpolicy[state]\u001b[38;5;241m.\u001b[39mget)\n",
      "File \u001b[1;32mc:\\Users\\javit\\Documents\\GitHub\\proyecto-bath\\RL\\W3\\racetrack_env.py:239\u001b[0m, in \u001b[0;36mRacetrackEnv.render\u001b[1;34m(self, sleep_time)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Sleep if desired.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (sleep_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) :\n\u001b[1;32m--> 239\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep_time)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "generate_episode(env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596a636f96ae5b76eb19c30b5ecee0ab",
     "grade": true,
     "grade_id": "cw2_racetrack_mc",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your MC agent here.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "882b7898f91d6277e8fde861c7b3f32c",
     "grade": false,
     "grade_id": "cell-372290cf2c9c7f09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plotting a Learning Curve\n",
    "\n",
    "Below, we call a function that takes your `mc_rewards` list and uses it to plot a learning curve. This graph shows the average undiscounted return earned by your agents each episode.\n",
    "\n",
    "We have included results from a correct implementation to compare your results to. Your learning curve should be close to this example one, but do not worry it does not match it exactly as there will naturally be some variation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbd1e202cbe799b9a0c167c83f0cd0ed",
     "grade": false,
     "grade_id": "cell-be9d8a57636c5b72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from racetrack_env import plot_results\n",
    "from racetrack_env import simple_issue_checking\n",
    "\n",
    "# Checking MC Control Results for Obvious Issues.\n",
    "simple_issue_checking(mc_rewards)\n",
    "\n",
    "# Plotting MC Control Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_results(mc_rewards = mc_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
